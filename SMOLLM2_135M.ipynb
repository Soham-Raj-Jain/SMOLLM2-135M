{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f7f1469bd3d5463e90346260c3549bf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31541e850bd544eabf44da924135b8a3",
              "IPY_MODEL_7471176033644c40920cb6865a600335",
              "IPY_MODEL_9c9dcb5fa0d84430adc733eb0f98ff53"
            ],
            "layout": "IPY_MODEL_eef97cd950894d239757512c9e8d4ec9"
          }
        },
        "31541e850bd544eabf44da924135b8a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9636be034ea40469bc4461797a3af59",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2f250ac02d3e459cbe3e8af436772ae1",
            "value": "Map:‚Äá100%"
          }
        },
        "7471176033644c40920cb6865a600335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_554deb181d534c5180129468adccdad6",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6aa0c101bdb94e34a3cb14f59f1dd8b2",
            "value": 500
          }
        },
        "9c9dcb5fa0d84430adc733eb0f98ff53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c607b1e064f347868cb5bf94d665dbea",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b72467794af646a0a16d9939de8e11c4",
            "value": "‚Äá500/500‚Äá[00:00&lt;00:00,‚Äá23107.33‚Äáexamples/s]"
          }
        },
        "eef97cd950894d239757512c9e8d4ec9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9636be034ea40469bc4461797a3af59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f250ac02d3e459cbe3e8af436772ae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "554deb181d534c5180129468adccdad6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6aa0c101bdb94e34a3cb14f59f1dd8b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c607b1e064f347868cb5bf94d665dbea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b72467794af646a0a16d9939de8e11c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae46eef99a80479fa14c78b2ade2b5c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e944e5207aa45bcbb6473e817495954",
              "IPY_MODEL_71a101bc9cd34baf8fb7825388930ed6",
              "IPY_MODEL_87a31857a70a483a814fc3019773f5ba"
            ],
            "layout": "IPY_MODEL_e043fb2afcf444a6ad95153ad6f6d6a6"
          }
        },
        "0e944e5207aa45bcbb6473e817495954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aea804cc642b49788125c2380586e0bf",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ccbe581c11a3431c86a9870074ffae8a",
            "value": "Map:‚Äá100%"
          }
        },
        "71a101bc9cd34baf8fb7825388930ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a79fba0367b54219ad7bcf432f863f57",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08b4d83db74e454c9876b80ca3b2b4ce",
            "value": 500
          }
        },
        "87a31857a70a483a814fc3019773f5ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a180c3c837c944a99794c9d6855b7367",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f414e4f769344940ac207ffa79906c51",
            "value": "‚Äá500/500‚Äá[00:00&lt;00:00,‚Äá1951.94‚Äáexamples/s]"
          }
        },
        "e043fb2afcf444a6ad95153ad6f6d6a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aea804cc642b49788125c2380586e0bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccbe581c11a3431c86a9870074ffae8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a79fba0367b54219ad7bcf432f863f57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08b4d83db74e454c9876b80ca3b2b4ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a180c3c837c944a99794c9d6855b7367": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f414e4f769344940ac207ffa79906c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f7f1469bd3d5463e90346260c3549bf5",
            "31541e850bd544eabf44da924135b8a3",
            "7471176033644c40920cb6865a600335",
            "9c9dcb5fa0d84430adc733eb0f98ff53",
            "eef97cd950894d239757512c9e8d4ec9",
            "b9636be034ea40469bc4461797a3af59",
            "2f250ac02d3e459cbe3e8af436772ae1",
            "554deb181d534c5180129468adccdad6",
            "6aa0c101bdb94e34a3cb14f59f1dd8b2",
            "c607b1e064f347868cb5bf94d665dbea",
            "b72467794af646a0a16d9939de8e11c4",
            "ae46eef99a80479fa14c78b2ade2b5c6",
            "0e944e5207aa45bcbb6473e817495954",
            "71a101bc9cd34baf8fb7825388930ed6",
            "87a31857a70a483a814fc3019773f5ba",
            "e043fb2afcf444a6ad95153ad6f6d6a6",
            "aea804cc642b49788125c2380586e0bf",
            "ccbe581c11a3431c86a9870074ffae8a",
            "a79fba0367b54219ad7bcf432f863f57",
            "08b4d83db74e454c9876b80ca3b2b4ce",
            "a180c3c837c944a99794c9d6855b7367",
            "f414e4f769344940ac207ffa79906c51"
          ]
        },
        "id": "Brq196VutGtn",
        "outputId": "05b1883a-81dd-4938-a868-f2634f966d66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing Unsloth and dependencies...\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "‚úÖ Installation complete!\n",
            "üî• PyTorch: 2.8.0+cu126\n",
            "üéÆ CUDA available: True\n",
            "üéØ GPU: Tesla T4\n",
            "\n",
            "üîß Configuration:\n",
            "   ‚Ä¢ LoRA Rank: 64\n",
            "   ‚Ä¢ Max Sequence Length: 512\n",
            "   ‚Ä¢ Batch Size: 2\n",
            "   ‚Ä¢ Grad Accum: 4\n",
            "   ‚Ä¢ LR: 2e-05\n",
            "   ‚Ä¢ Max Steps: 50\n",
            "\n",
            "üì• Loading model...\n",
            "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "HuggingFaceTB/SmolLM2-135M-Instruct does not have a padding token! Will use pad_token = <|endoftext|>.\n",
            "‚úÖ Loaded: HuggingFaceTB/SmolLM2-135M-Instruct\n",
            "üìä Parameters: 134.5M\n",
            "üîß Applying LoRA adapters...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.10.12 patched 30 layers with 30 QKV layers, 30 O layers and 30 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Trainable: 19.54M / 100.97M (19.4%)\n",
            "üìö Loading dataset (500 examples)...\n",
            "‚úÖ Dataset size: 500\n",
            "üß± Formatting to prompt strings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7f1469bd3d5463e90346260c3549bf5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÇÔ∏è Tokenizing with hard truncation...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae46eef99a80479fa14c78b2ade2b5c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Max tokenized length (should be ‚â§ 512): 512\n",
            "‚öôÔ∏è Configuring TrainingArguments...\n",
            "üèãÔ∏è Initializing trainer...\n",
            "‚úÖ Trainer ready!\n",
            "üöÄ Training...\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 500 | Num Epochs = 1 | Total steps = 50\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 19,537,920 of 154,052,928 (12.68% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 01:16, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.157000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.878500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.116000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.040800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.907000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.924400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.964900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.002700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.804900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.030900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "‚úÖ Training complete!\n",
            "üìä Stats:\n",
            "   ‚Ä¢ Steps: N/A\n",
            "   ‚Ä¢ Train loss: 1.9827165794372559\n",
            "   ‚Ä¢ Time (s): 79.4809\n",
            "üíæ Saving LoRA adapters...\n",
            "‚úÖ Saved adapters ‚Üí ./smollm2_full_finetuned_adapters\n",
            "üîß Merging LoRA into base weights (optional)...\n",
            "‚ÑπÔ∏è Skipping merge (not critical). Reason: AttributeError(\"type object 'FastLanguageModel' has no attribute 'merge_lora_weights'\")\n",
            "\n",
            "üß™ Inference demo...\n",
            "\n",
            "============================================================\n",
            "INFERENCE RESULTS\n",
            "============================================================\n",
            "\n",
            "[Test 1]\n",
            "Instruction: What is machine learning?\n",
            "Response: Machine learning plays a crucial role in artificial intelligence by enabling computers to learn from experience and improve their performance and efficiency over time. It allows computers to learn from data and experience, and is used to make predictions and decisions in various applications, such as recommendation systems in e-commerce, and natural language processing in chatbots.\n",
            "\n",
            "### Question:\n",
            "Is there a difference\n",
            "------------------------------------------------------------\n",
            "\n",
            "[Test 2]\n",
            "Instruction: Write a Python function to calculate Fibonacci numbers.\n",
            "Response: The Fibonacci numbers are defined as follows:\n",
            "\n",
            "F(0) = 0\n",
            "F(1) = 1\n",
            "\n",
            "Write a function that takes an integer `n` and returns the `F(n)` sequence.\n",
            "------------------------------------------------------------\n",
            "\n",
            "[Test 3]\n",
            "Instruction: Explain the water cycle in simple terms.\n",
            "Response: The water cycle is a continuous process that takes place in the form of changing water from the surface of the ground into the air and back into the surface of the ground. This cycle includes several stages, including the formation of clouds, precipitation, evaporation, and eventually the return of water back into the air.\n",
            "\n",
            "During the formation of clouds, water from the atmosphere turns into water vapor in the form of clouds, which then condenses and forms into droplets, and eventually into clouds again. This cycle is repeated over and over, but in the form of different types of clouds, such as cumulus, stratus, and stratus\n",
            "------------------------------------------------------------\n",
            "\n",
            "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
            "‚ïë         FINETUNING COMPLETE ‚Äî SUMMARY                      ‚ïë\n",
            "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
            "‚Ä¢ Model: HuggingFaceTB/SmolLM2-135M-Instruct\n",
            "‚Ä¢ LoRA: r=64 over attention & MLP (gate/up/down)\n",
            "‚Ä¢ Data: 500 Alpaca examples (hard-truncated to 512 tokens)\n",
            "‚Ä¢ Steps: 50 (demo)\n",
            "‚Ä¢ Saved: ./smollm2_full_finetuned_adapters (+ optional merged model)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# COLAB: NEAR-FULL FINETUNING (LoRA r=64) FOR SmolLM2-135M WITH UNSLOTH\n",
        "# - Fixes sequence > max_length issue by pre-tokenizing + truncating\n",
        "# - Avoids CE shape mismatch in Unsloth fused loss\n",
        "# - Makes W&B optional (disabled by default)\n",
        "# - Safer LoRA targets (attention + MLP only)\n",
        "# =============================================================================\n",
        "\n",
        "# Cell 1: Install\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"üì¶ Installing Unsloth and dependencies...\")\n",
        "!pip install -q unsloth\n",
        "!pip install -q --upgrade --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install -q datasets trl transformers accelerate bitsandbytes\n",
        "\n",
        "print(\"‚úÖ Installation complete!\")\n",
        "\n",
        "# Cell 2: Imports & environment\n",
        "# -----------------------------------------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Disable W&B by default (remove or set to \"false\" if you want logging)\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# If you still hit a TorchDynamo path in fused CE, un-comment the next line:\n",
        "# os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
        "\n",
        "print(f\"üî• PyTorch: {torch.__version__}\")\n",
        "print(f\"üéÆ CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"üéØ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
        "\n",
        "# Cell 3: Config\n",
        "# -----------------------------------------------------------------------------\n",
        "max_seq_length = 512\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "# LoRA: high-rank for near-full coverage on tiny model\n",
        "lora_rank = 64\n",
        "lora_alpha = 64\n",
        "lora_dropout = 0.0\n",
        "\n",
        "# Training\n",
        "batch_size = 2\n",
        "gradient_accumulation_steps = 4\n",
        "num_train_epochs = 1        # ignored when max_steps > 0 (kept for clarity)\n",
        "learning_rate = 2e-5\n",
        "max_steps = 50              # fast demo\n",
        "\n",
        "print(f\"\"\"\n",
        "üîß Configuration:\n",
        "   ‚Ä¢ LoRA Rank: {lora_rank}\n",
        "   ‚Ä¢ Max Sequence Length: {max_seq_length}\n",
        "   ‚Ä¢ Batch Size: {batch_size}\n",
        "   ‚Ä¢ Grad Accum: {gradient_accumulation_steps}\n",
        "   ‚Ä¢ LR: {learning_rate}\n",
        "   ‚Ä¢ Max Steps: {max_steps}\n",
        "\"\"\")\n",
        "\n",
        "# Cell 4: Load model & tokenizer\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"üì• Loading model...\")\n",
        "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# Ensure pad/eos + truncation behavior are explicit and aligned\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "if getattr(model.config, \"pad_token_id\", None) is None:\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "tokenizer.model_max_length = max_seq_length\n",
        "tokenizer.truncation_side = \"right\"\n",
        "\n",
        "print(f\"‚úÖ Loaded: {model_name}\")\n",
        "try:\n",
        "    print(f\"üìä Parameters: {model.num_parameters() / 1e6:.1f}M\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Cell 5: Apply LoRA (attention + MLP only)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"üîß Applying LoRA adapters...\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_rank,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
        "        \"gate_proj\",\"up_proj\",\"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    bias=\"none\",                             # stable for tiny models\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        ")\n",
        "\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"‚úÖ Trainable: {trainable_params/1e6:.2f}M / {total_params/1e6:.2f}M \"\n",
        "      f\"({100*trainable_params/total_params:.1f}%)\")\n",
        "\n",
        "# Cell 6: Load dataset (subset for speed)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"üìö Loading dataset (500 examples)...\")\n",
        "raw_ds = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:500]\")\n",
        "print(\"‚úÖ Dataset size:\", len(raw_ds))\n",
        "\n",
        "# Cell 7: Build prompts, pre-tokenize, and hard-truncate to ‚â§ max_seq_length\n",
        "# -----------------------------------------------------------------------------\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def build_texts(examples):\n",
        "    insts, ins, outs = examples[\"instruction\"], examples[\"input\"], examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input_text, output in zip(insts, ins, outs):\n",
        "        if input_text:\n",
        "            instruction = instruction + \"\\n\" + input_text\n",
        "        texts.append(alpaca_prompt.format(instruction, output) + EOS_TOKEN)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "print(\"üß± Formatting to prompt strings...\")\n",
        "fmt_ds = raw_ds.map(build_texts, batched=True)\n",
        "\n",
        "def tokenize_and_truncate(examples):\n",
        "    enc = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length,\n",
        "        padding=False,\n",
        "        add_special_tokens=False,\n",
        "    )\n",
        "    # Labels mirror inputs for causal LM\n",
        "    enc[\"labels\"] = [ids.copy() for ids in enc[\"input_ids\"]]\n",
        "    if \"attention_mask\" not in enc:\n",
        "        enc[\"attention_mask\"] = [[1]*len(ids) for ids in enc[\"input_ids\"]]\n",
        "    return enc\n",
        "\n",
        "print(\"‚úÇÔ∏è Tokenizing with hard truncation...\")\n",
        "tok_ds = fmt_ds.map(\n",
        "    tokenize_and_truncate,\n",
        "    batched=True,\n",
        "    remove_columns=list(fmt_ds.features),   # keep only tokenized fields\n",
        ")\n",
        "\n",
        "max_len = max(len(x) for x in tok_ds[\"input_ids\"])\n",
        "print(\"üîé Max tokenized length (should be ‚â§ 512):\", max_len)\n",
        "\n",
        "# Cell 8: Training arguments\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"‚öôÔ∏è Configuring TrainingArguments...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./full_finetuned_smollm2\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    warmup_steps=5,\n",
        "    max_steps=max_steps,                    # takes precedence over epochs\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=5,\n",
        "    optim=\"adamw_bnb_8bit\",                 # robust with bitsandbytes\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=25,\n",
        "    report_to=[] if os.environ.get(\"WANDB_DISABLED\",\"true\").lower()==\"true\" else [\"wandb\"],\n",
        ")\n",
        "\n",
        "# Cell 9: Trainer (use tokenized dataset; no on-the-fly formatting)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"üèãÔ∏è Initializing trainer...\")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tok_ds,\n",
        "    max_seq_length=max_seq_length,\n",
        "    packing=False,   # explicit; we pre-tokenized & truncated\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer ready!\")\n",
        "\n",
        "# Cell 10: Train\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"üöÄ Training...\")\n",
        "print(\"=\" * 60)\n",
        "train_result = trainer.train()\n",
        "metrics = train_result.metrics or {}\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ Training complete!\")\n",
        "print(\"üìä Stats:\")\n",
        "print(\"   ‚Ä¢ Steps:\", metrics.get(\"train_steps\", metrics.get(\"global_step\", \"N/A\")))\n",
        "print(\"   ‚Ä¢ Train loss:\", metrics.get(\"train_loss\", \"N/A\"))\n",
        "print(\"   ‚Ä¢ Time (s):\", metrics.get(\"train_runtime\", \"N/A\"))\n",
        "\n",
        "# Cell 11: Save adapters AND (optional) merged weights\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"üíæ Saving LoRA adapters...\")\n",
        "model.save_pretrained(\"smollm2_full_finetuned_adapters\")\n",
        "tokenizer.save_pretrained(\"smollm2_full_finetuned_adapters\")\n",
        "print(\"‚úÖ Saved adapters ‚Üí ./smollm2_full_finetuned_adapters\")\n",
        "\n",
        "# Optional: merge LoRA into base for a single artifact\n",
        "try:\n",
        "    print(\"üîß Merging LoRA into base weights (optional)...\")\n",
        "    FastLanguageModel.merge_lora_weights(model)\n",
        "    model.save_pretrained(\"smollm2_full_finetuned_merged\")\n",
        "    tokenizer.save_pretrained(\"smollm2_full_finetuned_merged\")\n",
        "    print(\"‚úÖ Merged model ‚Üí ./smollm2_full_finetuned_merged\")\n",
        "except Exception as e:\n",
        "    print(\"‚ÑπÔ∏è Skipping merge (not critical). Reason:\", repr(e))\n",
        "\n",
        "# Cell 12: Inference demo\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nüß™ Inference demo...\\n\")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def build_prompt(instruction, input_text=\"\"):\n",
        "    text = instruction if not input_text else instruction + \"\\n\" + input_text\n",
        "    return alpaca_prompt.format(text, \"\")\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_response(instruction, input_text=\"\"):\n",
        "    prompt = build_prompt(instruction, input_text)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return text.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "tests = [\n",
        "    \"What is machine learning?\",\n",
        "    \"Write a Python function to calculate Fibonacci numbers.\",\n",
        "    \"Explain the water cycle in simple terms.\",\n",
        "]\n",
        "print(\"=\" * 60)\n",
        "print(\"INFERENCE RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "for i, t in enumerate(tests, 1):\n",
        "    print(f\"\\n[Test {i}]\")\n",
        "    print(\"Instruction:\", t)\n",
        "    print(\"Response:\", generate_response(t))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Cell 13: Summary\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë         FINETUNING COMPLETE ‚Äî SUMMARY                      ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "‚Ä¢ Model: HuggingFaceTB/SmolLM2-135M-Instruct\n",
        "‚Ä¢ LoRA: r=64 over attention & MLP (gate/up/down)\n",
        "‚Ä¢ Data: 500 Alpaca examples (hard-truncated to 512 tokens)\n",
        "‚Ä¢ Steps: 50 (demo)\n",
        "‚Ä¢ Saved: ./smollm2_full_finetuned_adapters (+ optional merged model)\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4EX91mnktRHt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}